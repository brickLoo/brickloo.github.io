---
date: '2024-03-11'
title: 'Neural Networks'
weight: 5
math: true
url: '/docs/pr/5'
draft: true
---

笔记作者：BrickLoo

## Artificial Neural Network

回顾前一章节 Linear Classification 的末尾，我们提到了线性分类器在几何学上的非凹特性。也就是说，对于决策区域的形状较为复杂，例如出现凹陷或者是需要使用异或逻辑的时候，线性分类器就显得比较无能为力了。对于这个问题，改进的思路自然是引入非线性函数。但接下来的两个难点是，如何引入非线性函数，引入什么非线性函数。即使是一些存在异或逻辑的决策区域，或者有一些凹陷的决策区域，只要找到合适的非线性函数其实都是有可能拟合的，但是这并不优雅——因为通常来说并不知道哪种非线性函数拟合得比较好，即使我们找一个完备的基函数集合使其能表示出任何一种非线性函数，也有太多的参数要估计，我们难以训练。

不妨让生物科学的发展为我们打开视野。观察人的神经元，它从其他神经元那里接受输入，并在满足条件的情况下将激活信号传递给下一个神经元。这与我们前面所讨论的线性模型其实是很相似的：接受输入，按权重向加，然后判断类别。但它的不同点在于，它会将自己的类别判断传递给下一个“单元”。那线性模型是否也可以作为一个个“单元”来组成类似的结构，以弥补线性模型主要的局限性？答案是肯定的。仔细思考一下，从加权求和到判定类别，其实我们是经过了一个非线性的处理。转换为数学语言来描述，这个非线性的处理可以描述为
$$f(x)=\begin{cases}
   +1 & x \gt 0 \newline
   -1 & x \lt 0
\end{cases}$$
虽然这样的非线性处理看着非常简单，但是实际上只要我们能将非线性的信息传递下去，使得整个分类器不完全是线性的，我们就能使用这些“单元”拼凑出更强大的分类器来解决各种非线性问题。

### XOR Example

以异或问题为例，如果我们希望能够将 $(0,1)$ 和 $(1,0)$ 的情况与 $(0,0)$ 和 $(1,1)$ 分离开，我们不妨：
- 先用一个“单元”将 $(0,0)$ 和其他分离开，如 $(0,0)$ 点输出 $1$，其他输出 $0$；
- 再用一个“单元”将 $(1,1)$ 和其他分离开，如 $(1,1)$ 点输出 $1$，其他输出 $0$。
- 这时，我们将这些非线性的分类信息传递给下一个“单元”，下一个“单元”只需要判断是否有被任意一个分离开即可，如判断两者之和为 $0$ 还是为 $1$。

这样，异或问题就通过线性函数和简单的非线性函数组成的“单元”解决了。

仔细回想这个过程，通过多个“单元”并列，我们实现了多个线性决策边界的联动，这有点化直为曲的意思在；同时，通过将“单元”的非线性信息逐级传递，层层叠加，我们还能实现复杂的逻辑。即便这背后的数学原理和参数形式依旧比较简单，使用这种“单元”组成的模型来解决复杂的问题已经“理论可行”了。线性模型当初的局限早已不在话下，我们只需要训练模型中的参数来使其具有合适的逻辑关系即可。不得不说，这种解决方案是非常优雅的。我们不妨为其起一个漂亮的名字——**神经网络**。每一个“单元”不妨就叫做 **“神经元”**，其中非线性的“分类函数”不妨就类比神经元的激活，称为 **“激活函数”**。

### Activation Function

这个解决方案看似完整，实际上我们还需要详细探讨如何训练的问题。

在前一章节里，我们简单提及了广义线性模型的 LMS 训练方法，虽然看起来也有非线性的成分，但是这里还是略微有些区别——在这里，我们每一个神经元应当作为完整的单元考虑，激活函数也需要考虑上。如果将激活函数用 $f(x)$ 表示，那么，一个神经元所对应的函数应当是
$$g(x)=f(\sum^d_{i=1} w_i\phi_i(x))$$
如果我们希望使用 LMS 方法来训练，则需要求出误差的梯度
$$\begin{align*}
\nabla_{w_i}L(x)&=2\big(g(x)-c\big)\cdot\frac{\partial g}{\partial w_i}\newline
&=2\big(g(x)-c\big)\cdot\frac{\partial g}{\partial f}\frac{\partial f}{\partial w_i}
\end{align*}$$
这可不好办，因为前面所举例的激活函数不是连续的，即使能求导也是得到数值为 $0$ 的结果，导致我们无法使用 LMS 方法进行梯度下降。所以，我们不妨换一个激活函数，让它既能通过求导来进行模型参数的训练，形态也类似于原本案例中的激活函数，能够将正值和负值分为两类。

对于满足这两个要求的非线性函数，其中一种比较有名的构造叫做 Sigmoid 函数，它的数学表达是
$$f(x)=\frac{1}{1+e^{-x}}$$
它尝试将正值和负值分别分类为 $1$ 和 $0$，但在临界点 $x=0$ 的附近进行了平滑的过渡，有种“是又不完全是”的感觉，使得模型参数能够通过梯度下降的 LMS 进行训练。