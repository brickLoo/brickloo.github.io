---
date: '2024-05-23'
title: '对抗样本生成技术'
weight: 1
math: true
prev: '/docs/论文阅读笔记'
url: '/docs/papers-notes/adv-attacks'
---

> 笔记作者：BrickLoo  
> 声明：本文旨在简要地介绍对抗样本并串连地介绍一些常见的对抗样本生成技术。内容基于个人理解，仅供参考，可能不适合作为正式场合中的引用。若发现理解错误的地方欢迎指出或提交修改。  
> 补充：[此 Github 项目](https://github.com/Harry24k/adversarial-attacks-pytorch)中包含了较全的对抗样本生成方法的实现与论文引用，适合作为进一步阅读研究的材料。

## 对抗样本

### 什么是对抗样本

理想状况下，我们假设会存在这样一种样本，它是通过在正常样本上添加微小扰动得到的，人类无法察觉它们与正常样本的差别，但它们无法被人工智能模型正确地分类。

### 为什么需要研究对抗样本

- 如果我们找到了这样的样本，我们可以逃避人工智能模型的“监控”，保护样本数据的隐私；
- 恶意的对手可能会研究对抗样本来攻击人工智能模型，在关键时刻造成严重损失，具有安全隐患，因此我们需要知己知彼地对模型施加保护。

### 数学描述

为了方便后文的介绍，这里首先约定一些问题中常见的数学符号和含义：

|符号|含义|
|---|---|
|$x$|正常样本|
|$\delta$|施加在样本上的微小扰动|
|$x^\prime$|对抗样本，数值上等于 $x+\delta$|
|$f$|人工智能模型，当输入 $x$ 时输出 $f(x)$|
|$\epsilon$|可接受的扰动阈值|
|$\alpha$|单次扰动的大小设定|
|$\eta$|不超过 $\epsilon$ 的随机扰动|

由于并非所有数学符号的相关含义都在前文中被提及，因此不用先完全搞明白这张表，可以等到后面出现的时候再回头来回顾。

仔细回顾定义，我们会发现问题可以描述为两个目标：
1. 为了让人类无法察觉它们与正常样本的差别，我们需要尽可能地减少他们之间的差距，即 $\text{minimize }\lVert\delta\rVert$
2. 对抗样本无法被分类正确，即 $f(x^\prime) \neq f(x)$

值得注意的是，这里我们使用 $\lVert\delta\rVert$ 来描述样本之间的差距，而不是 $\delta$。因为 $\delta$ 是能直接与 $x$ 进行运算的，它是一个记录了每个像素点的扰动大小的矩阵。而评估差距大小的时候，我们需要通过求范数将其转换成一个数值。在这里，范数可以采用多种方式计算，包括：
- $L_1$ 范数：考虑施加了扰动的像素点数量
- $L_2$ 范数：考虑正常样本与对抗样本之间的欧氏距离
- $L_\infin$ 范数：考虑对像素点扰动大小的上限

其中，最常用的计算方法是 $L_\infin$ 范数。

## 对抗样本生成技术

### FGSM

> **Explaining and Harnessing Adversarial Examples**  
> Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy  
> [arXiv](https://arxiv.org/abs/1412.6572) &nbsp; [Semantic Scholar](https://www.semanticscholar.org/paper/Explaining-and-Harnessing-Adversarial-Examples-Goodfellow-Shlens/bee044c8e8903fb67523c1f8c105ab4718600cdb)

该方法的全称为“Fast Gradient Signed Method”。为了简化问题，FGSM 方法将第一个优化目标转化为约束，做出以下假设：
- 当 $\lVert\delta\rVert\lt\epsilon$，认为扰动效果对人不产生影响，人类无法察觉。

而对于第二个目标，我们可以从反面的角度考虑：
- 要使模型分类得正确，就是要使其在训练过程中逐渐减小损失；
- 那么要使模型分类得不正确，自然就是构造样本来增加模型的损失。

因此，方法名中“Gradient”的含义，就是需要计算模型在正常样本处的损失梯度，并沿着梯度方向来改变正常样本，以得到对抗样本。只不过平时训练时我们是以参数为变量，而此时我们以样本为变量。

不过，这个方向并不是对于样本来说最精确的梯度方向。对于每一个像素点，我们计算完梯度之后只利用它的正负性，并直接施加大小为 $\epsilon$ 的扰动。这也就是方法名中“Signed”的含义。这是因为直观来说，扰动越大，损失也会越大，因此简单起见我们直接按符合约束的最大扰动来操作就行。

将以上思路用数学语言来描述可以写成：
$$x^\prime=x+\epsilon*\mathrm{sign}\big(\nabla_xJ(\theta,x,y)\big)$$
其中 $J$ 是模型 $f$ 的损失，$\theta$ 是模型 $f$ 的参数。

### BIM

> **Adversarial examples in the physical world**  
> Alexey Kurakin, Ian Goodfellow, Samy Bengio  
> [arXiv](https://arxiv.org/abs/1607.02533) &nbsp; [Semantic Scholar](https://www.semanticscholar.org/paper/Adversarial-examples-in-the-physical-world-Kurakin-Goodfellow/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b)

该方法的全称为“Basic Iterative Method”，它可以理解为 FGSM 的迭代版本。还记得在 FGSM 中，我们做出了“扰动越大，损失越大”的假设——然而，这个规律只是通常成立，却并不绝对。例如，对于某些像素点，在扰动大小从 $0$ 到 $\epsilon$ 的变化过程中，模型的损失事实上可能会先增大后减小。

因此，我们可以从粗粒度到细粒度，不一次性地施加大小为 $\epsilon$ 的扰动，而是用另一个超参数 $\alpha$ 来控制单步扰动的大小。通过这种步长为 $\alpha$ 的单步 FGSM 后，我们可以继续迭代，找到下一步的对抗样本。在多次迭代的过程中，相比于直接使用 $\epsilon$ 作为扰动大小，BIM 赋予了攻击方法“回头”的能力，这使得生成的对抗样本会具有更好的攻击效果。

这个过程用数学语言来描述就是：
$$x^\prime_0=x,\space x^\prime_{n+1}=\mathrm{Clip}_{x,\epsilon}\{x^\prime_n+\alpha*\mathrm{sign}\big(\nabla_xJ(\theta,x^\prime_n,y)\big)\}$$

此处的 $\mathrm{Clip}$ 操作是为了防止迭代生成攻击样本的迭代过程中，累积施加的扰动超出可接受的阈值。

在原文中，作者进一步提出了 BIM 的方法变体“Iterative Least-Likely Class Method”。作者首先选择了样本最不可能被预测成为的类别
$$y_{LL}=\argmin_y\{p(y \mid x)\}$$
然后迭代地加强模型将对抗样本样本预测成此类别的可能性
$$x^\prime_0=x,\space x^\prime_{n+1}=\mathrm{Clip}_{x,\epsilon}\{x^\prime_n-\alpha*\mathrm{sign}\big(\nabla_xJ(\theta,x^\prime_n,y)\big)\}$$
注意，这里的加减符号发生了转换，而变得与模型的训练过程更加相似——只不过选择了与正常训练时不同的标签，且平时训练模型时我们是以参数为变量，而此时我们以样本为变量。

作者表明，当指定了一个特定的目标类别，即使选择的是最不可能的类别，对抗样本也更容易在类别较多的任务如 ImageNet 数据集上体现出攻击性。这大概是因为如果我们不指定一个特定的目标类别，原类别上减少的可能性可能会比较均匀地摊到其他类别上，导致模型准确度下降得不明显。

### PGD

> **Towards Deep Learning Models Resistant to Adversarial Attacks**  
> Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu  
> [arXiv](https://arxiv.org/abs/1706.06083) &nbsp; [Semantic Scholar](https://www.semanticscholar.org/paper/Towards-Deep-Learning-Models-Resistant-to-Attacks-Madry-Makelov/7aa38b85fa8cba64d6a4010543f6695dbf5f1386)

该方法的全称为“Projected Gradient
Descent”，它的提出原本旨在以一种更加完备的方式来总结前人的对抗样本生成工作。其中的“Projected”在 BIM 方法中对应的是 $\mathrm{Clip}$ 操作，将样本“投影”回可接受的阈值之内。

在这个工作中，作者发现，如果对原始样本首先施加一个随机的扰动作为 BIM 等方法的起点，有时生成的对抗样本会拥有更好的攻击性能。引入的随机性正是 PGD 方法相比前面方法的区别。

引入随机性后，方法用数学语言来描述可以写成：
$$x^\prime_0=x+\eta,\space x^\prime_{n+1}=\prod_{x,\epsilon}\{x^\prime_n-\alpha*\mathrm{sign}\big(\nabla_xJ(\theta,x^\prime_n,y)\big)\}$$

其中 $\eta$ 是可接受的扰动阈值内的随机扰动，它帮助我们找到一个新的起点来实施对抗样本生成方法。$\prod_{x,\epsilon}$ 表示每一次迭代中的投影操作，它帮助我们将生成的对抗样本约束在可接受的扰动阈值之内。

### CW

> **Towards Evaluating the Robustness of Neural Networks**  
> Nicholas Carlini, David Wagner  
> [arXiv](https://arxiv.org/abs/1608.04644) &nbsp; [Semantic Scholar](https://www.semanticscholar.org/paper/Towards-Evaluating-the-Robustness-of-Neural-Carlini-Wagner/df40ce107a71b770c9d0354b78fdd8989da80d2f)

该方法的名字取自提出方法的两位作者的姓氏首字母。比起前面的方法将对抗样本生成的第一个目标按照约束的形式处理，CW 方法实打实地将其视为优化目标，并使用加权求和的方式将两个目标统一到同一个式子中，从整体的视角将问题视为一个损失最小化问题。即，
$$\text{minimize }\lVert\delta\rVert+\lambda L(x+\delta)$$
其中，$\lambda$ 就是用于控制权重的超参数，$L$ 是构造出来评估对抗样本优劣的损失函数（注意不是模型预测准确度的损失函数）。在论文中，作者尝试了多种 $L$ 的设计，并通过实验表明性能相对最好的设计是
$$L(x^\prime)=\bigg(\max_{i\neq t}\big(Z(x^\prime)_i\big)-Z(x^\prime)_t\bigg)^+$$
式中，$Z$ 表示模型 $f$ 在经过最后的 softmax 层之前输出的 logits，下标 $i$ 和 $t$ 分别代表任意类别对应的 logits 分量和目标类别对应的 logits 分量。这个损失函数取目标类别与最可能类别之间的 logits 值差距作为损失，迫使模型对于对抗样本降低当前最可能类别的可能性，同时提高目标类别的可能性，直到目标类别就是最可能的类别为止。这种设计其实颇有种 BIM 及其变体方法合二为一的感觉。

除此之外，值得注意的是，虽然我们在 CW 方法中将扰动大小视为优化目标而不是约束，但我们在采用梯度下降来找到最优对抗样本的过程中，我们还是有一个隐性的约束——图像中每一个像素点的合法取值通常应该处于 $0$ 到 $255$ 之间（根据不同的图像表示方式有时也可能是处于 $0$ 到 $1$ 之间）。考虑到截断性操作可能会造成误差传递或者无法求导等问题，作者并不像前面的几种方法直接将对抗样本的取值截取到合法范围内，而是采取了换元的技巧，将对抗样本重新使用变量 $w$ 来表示，变为
$$x^\prime=\frac{1}{2}\big(\tanh(w)+1\big)\cdot 255$$
这样，通过新的变量 $w$ 表示对抗样本时，我们就能正常求导，同时不用再担心对抗样本取值超出合法范围的问题了。

### AutoAttack

> **Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks**  
> Francesco Croce, Matthias Hein  
> [arXiv](https://arxiv.org/abs/2003.01690) &nbsp; [Semantic Scholar](https://www.semanticscholar.org/paper/Reliable-evaluation-of-adversarial-robustness-with-Croce-Hein/18939eadc9c4460c8385e0591cde214a1ead067b)

PGD 方法其实仍有许多待改进的地方。在这个工作中，作者就将 PGD 改进为 Auto-PGD（简称 APGD），并融合了同期其他两个不使用梯度信息的对抗样本生成方法，整理成一个多方法的模型性能评估策略 AutoAttack，并成为了当时的 SOTA 方法。

PGD 如果理解为模型训练中梯度下降的逆向过程，那么模型训练中使用到的技巧也就能迁移到 PGD 中了，这里作者正是利用了这个思路来实现了 PGD 方法的改进。作者的改进主要在于这两个角度
- 首先，对样本按照梯度信息进行迭代的过程，其实类似于模型训练中的随机梯度下降 SGD，因此也可以引入动量机制来避免优化过程中的振荡问题；
- 另外，参考训练过程中优化步长逐步减小的策略，作者这里也提出了一套随着对抗样本生成过程逐渐减小单次扰动大小 $\alpha$ 的方案，缓解了这个超参数大了不精，小了太慢的问题。也正是因为避免了这个超参数的选取问题，作者为这个方法冠以 auto 的前缀。

除此之外，对于一开始提到的对抗样本生成的第二个目标，作者沿用了 CW 方法中的损失函数 $L$。作者指出，由于 logits 不具有缩放不变性，且模型通常会具有过度自信问题，如果选择了常用的交叉熵损失，对抗样本生成的过程中容易存在梯度消失难以优化等问题。同时，为了避免 CW 方法中的损失函数也出现这种问题，作者进一步改进了这个损失函数，使其具备缩放不变性。新的损失函数被作者称为 DLR：
$$DLR(x,y)=-\frac{z_y-\mathop{max}\limits_{i \neq y}\{z_i\}}{z_{\pi_1}-z_{\pi_3}}$$

式中，$z$ 是 logits 值，$\pi$ 及其下标表示降序排序中的位次信息。由于最大得分的错误类别只可能处于第一或第二的位次，作者通过第一位次和第三位次的 logits 值差异作为分母，能够使得最终的损失值落在 $(-1,1)$ 之间。此时即使对 logits 值进行等比例缩放，DLR 不会受到影响。

同时，作者也为有目标的攻击场景设计了替代损失函数 Targeted-DLR：
$$Targeted\text{-}DLR(x,y)=-\frac{z_y-z_t}{z_{\pi_1}-(z_{\pi_3}+z_{\pi_4})/2}$$
在原本的 DLR 中，如果目标类别的位次恰好处于第三位时，损失函数将恒为 $1$，进而梯度恒为 $0$，导致无法优化对抗样本。因此作者对分母进行了调整，以避免这个问题。

关于 AutoAttack 的详细分析，可以进一步阅读[这篇文章](https://brickloo.github.io/docs/papers-notes/AutoAttack/)。
